<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · Factorized Embeddings Documentation</title><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>Factorized Embeddings Documentation</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Description"><span>Description</span></a></li><li><a class="tocitem" href="#Public-Interface"><span>Public Interface</span></a></li></ul></li><li><a class="tocitem" href="guide/">Guide</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/Kontrabass2018/FactorizedEmbeddings/blob/main/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Factorized-Embeddings-method"><a class="docs-heading-anchor" href="#Factorized-Embeddings-method">Factorized Embeddings method</a><a id="Factorized-Embeddings-method-1"></a><a class="docs-heading-anchor-permalink" href="#Factorized-Embeddings-method" title="Permalink"></a></h1><h2 id="Description"><a class="docs-heading-anchor" href="#Description">Description</a><a id="Description-1"></a><a class="docs-heading-anchor-permalink" href="#Description" title="Permalink"></a></h2><p>Factorized Embeddings is described in this <a href="https://doi.org/10.1093/bioinformatics/btaa488">article</a> by Trofimov et al. 2020. This model is a self-supervised deep neural network that uses tensor factorization to simultaneously learn gene and sample representation spaces. This type of network is tailored to work with large RNA sequencing data, but can be applied to any type of large multivariate data. The FE model treats both factors (typically samples and gene) as factors contributing to characterizing the values (ie. gene expression data). This architecture generates sample representations that can be used for auxiliary tasks such as visualization or classification.</p><p>The code in this package is based on the first Factorized Embeddings analysis scripts according to the code provided by Trofimov et al. (2020) <a href="https://github.com/TrofimovAssya/Factorizedembeddings">GitHub link</a>. The demonstrated functionalities of Factorized Embeddings are implemented in the Julia programming language <a href="www.julialang.org">julialang.org</a> using the Flux library <a href="https://fluxml.ai/Flux.jl/stable/">Innes 2018</a>. The main features of the code include: •	Generation of low-dimensional sample embeddings that can be used for auxiliary tasks like 2D visualization or classification. •	Highly configurable models via a dictionary of hyperparameters provided by the user. •	During network optimization, models, learning curves, reconstruction performance, and, if possible, 2D sample embedding visualizations are recorded. •	Once trained, the model can infer new points. •	A functionality is also available for imputing the sample embedding space in 2D.</p><p>Model optimizations were tested using SGD with the ADAM optimizer (Kingma and Ba 2017) on GPUs (V100-SMX2, 32 GB) on servers with 64 GB RAM. It is important to note that this implementation requires access to a GPU for optimal performance. The code can be adapted for specific server constraints, such as reducing mini-batch sizes and limiting the number of network parameters. Details about the required package installations for GPU usage are also provided in this guide.</p><p><a href="assets/FE_schematic.png">Schematic of Factorized Embeddings</a></p><h2 id="Public-Interface"><a class="docs-heading-anchor" href="#Public-Interface">Public Interface</a><a id="Public-Interface-1"></a><a class="docs-heading-anchor-permalink" href="#Public-Interface" title="Permalink"></a></h2><h3 id="Main-methods"><a class="docs-heading-anchor" href="#Main-methods">Main methods</a><a id="Main-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Main-methods" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="FactorizedEmbeddings.generate_params" href="#FactorizedEmbeddings.generate_params"><code>FactorizedEmbeddings.generate_params</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generate_params(X_data::AbstractArray; 
                emb_size::Int=2, emb_size_2::Int=100, 
                nsteps_dim_redux::Int=1000, l2_val::Float64=1e-7, 
                fe_layers_size = [100, 50, 50]
           )</code></pre><p>Function that takes input hyper-parameters and outputs a dictonary. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Kontrabass2018/FactorizedEmbeddings/blob/5400387ec551c91d34b5539aff207974ba3babbe/src/FactorizedEmbeddings.jl#L129-L137">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="FactorizedEmbeddings.fit" href="#FactorizedEmbeddings.fit"><code>FactorizedEmbeddings.fit</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fit(X_data; dim_redux_size::Int=2, nsteps::Int=1000, l2::Float64=1e-7)</code></pre><p>This function instanciates a Factorized Embeddings model with default or imputed parameters. Then trains the model on the input data and returns the trained model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Kontrabass2018/FactorizedEmbeddings/blob/5400387ec551c91d34b5539aff207974ba3babbe/src/FactorizedEmbeddings.jl#L159-L163">source</a></section><section><div><pre><code class="nohighlight hljs">fit(X_data, FE_params::Dict)</code></pre><p>This function instanciates a Factorized Embeddings model imputed hyper-parameter dictionary. Then trains the model on the input data and returns the trained model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Kontrabass2018/FactorizedEmbeddings/blob/5400387ec551c91d34b5539aff207974ba3babbe/src/FactorizedEmbeddings.jl#L178-L182">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="FactorizedEmbeddings.fit_transform" href="#FactorizedEmbeddings.fit_transform"><code>FactorizedEmbeddings.fit_transform</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fit_transform(X_data; dim_redux_size::Int=2, nsteps::Int=1000, l2::Float64=1e-7, verbose::Int = 0)</code></pre><p>This function instanciates a Factorized Embeddings model with default or imputed parameters. Then trains the model on the input data and returns the dimensionality-reduced sample embedding.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Kontrabass2018/FactorizedEmbeddings/blob/5400387ec551c91d34b5539aff207974ba3babbe/src/FactorizedEmbeddings.jl#L209-L213">source</a></section><section><div><pre><code class="nohighlight hljs">fit_transform(X_data, FE_params::Dict;verbose::Int=0)</code></pre><p>This function instanciates a Factorized Embeddings model imputed hyper-parameter dictionary. Then trains the model on the input data and returns the dimensionality-reduced sample embedding.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Kontrabass2018/FactorizedEmbeddings/blob/5400387ec551c91d34b5539aff207974ba3babbe/src/FactorizedEmbeddings.jl#L220-L224">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="FactorizedEmbeddings.infer" href="#FactorizedEmbeddings.infer"><code>FactorizedEmbeddings.infer</code></a> — <span class="docstring-category">Function</span></header><section><div><p>infer(trained<em>FE, train</em>data, test<em>data, params</em>dict;verbose=0)</p><p>Infers new data with the pre-trained model. Input parameters. trained<em>FE: the pre-trained Flux DNN model. train</em>data: the training dataset. test<em>data: the test dataset.  params</em>dict: Dictionary of hyper-parameters that was set during the training phase.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Kontrabass2018/FactorizedEmbeddings/blob/5400387ec551c91d34b5539aff207974ba3babbe/src/FactorizedEmbeddings.jl#L230-L238">source</a></section></article><h3 id="Other-utility-methods"><a class="docs-heading-anchor" href="#Other-utility-methods">Other utility methods</a><a id="Other-utility-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Other-utility-methods" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="FactorizedEmbeddings.fit!" href="#FactorizedEmbeddings.fit!"><code>FactorizedEmbeddings.fit!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fit!(X_data, model, FE_params::Dict)</code></pre><p>This function uses the inputed Factorized Embeddings model with pre-defined and imputed hyper-parameter dictionary. Then trains the model on the input data and returns the trained model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Kontrabass2018/FactorizedEmbeddings/blob/5400387ec551c91d34b5539aff207974ba3babbe/src/FactorizedEmbeddings.jl#L194-L198">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="FactorizedEmbeddings.reset_embedding_layer" href="#FactorizedEmbeddings.reset_embedding_layer"><code>FactorizedEmbeddings.reset_embedding_layer</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function reset_embedding_layer!(FE_net, new_embed; cp_dev=gpu)</code></pre><p>This methods copies the imputed model and returns a model with inputed pre-defined embedding layer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Kontrabass2018/FactorizedEmbeddings/blob/5400387ec551c91d34b5539aff207974ba3babbe/src/FactorizedEmbeddings.jl#L79-L83">source</a></section></article><p>This function allows to initialise the sample embedding with a certain pre-defined embedding.</p><article class="docstring"><header><a class="docstring-binding" id="FactorizedEmbeddings.train!" href="#FactorizedEmbeddings.train!"><code>FactorizedEmbeddings.train!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">function train!(params, X, Y, model;verbose = 0)</code></pre><p>This methods trains a model with the X and Y training data and returns the trained model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Kontrabass2018/FactorizedEmbeddings/blob/5400387ec551c91d34b5539aff207974ba3babbe/src/FactorizedEmbeddings.jl#L96-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="FactorizedEmbeddings.FE_model" href="#FactorizedEmbeddings.FE_model"><code>FactorizedEmbeddings.FE_model</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">FE_model(params::Dict)</code></pre><p>This method takes as input a dictionary of hyperparameters instantiates and returns a Flux.Chain type DNN model.  This model can then be fed to the train! method to fit the model&#39;s parameters to the data. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Kontrabass2018/FactorizedEmbeddings/blob/5400387ec551c91d34b5539aff207974ba3babbe/src/FactorizedEmbeddings.jl#L52-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="FactorizedEmbeddings.prep_FE" href="#FactorizedEmbeddings.prep_FE"><code>FactorizedEmbeddings.prep_FE</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">prep_FE(data::Matrix, device=gpu; order = &quot;shuffled&quot;, verbose = 0)</code></pre><p>This method takes the data matrix as input and outputs the vectors X containing two vectors of samples indices and gene indices and Y containing the expression values of the sample and the gene.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Kontrabass2018/FactorizedEmbeddings/blob/5400387ec551c91d34b5539aff207974ba3babbe/src/FactorizedEmbeddings.jl#L23-L28">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="guide/">Guide »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Tuesday 14 January 2025 19:24">Tuesday 14 January 2025</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
